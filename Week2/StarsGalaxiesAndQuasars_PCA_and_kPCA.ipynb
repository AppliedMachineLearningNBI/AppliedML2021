{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of stars, galaxies, and quasars\n",
    "\n",
    "Using a query, you should try to obtain three data files each contains 10000 observations of stars, galaxies, and quasars, respectively, which are distinctly different objects.\n",
    "\n",
    "As this is data resuting from a (raw) query, it has got several missing/9999 values and other flaws to consider, before applying any further data analysis. You need to fix this!\n",
    "\n",
    "Thus, this exercise first consists of **Inspecting and Cleaning the data**.\n",
    "\n",
    "### Features:\n",
    "The data features / input variables (X) are:\n",
    "* ID:   unique object ID in the database\n",
    "* ra:   right ascension (coordinate)\n",
    "* dec:  declination (coordinate)\n",
    "* istar:        log-likelihood that the object is point-like, given by the pipeline run on the images\n",
    "* gmag:  magnitude in g-band\n",
    "* rmag: magnitude in r-band\n",
    "* imag: magnitude in i-band\n",
    "* zmag: magnitude in z-band\n",
    "* W1:   magnitude in W1-band (from AllWISE)\n",
    "* W2:   magnitude in W2-band (from AllWISE)\n",
    "* psfgmag:      PSF magnitude in g-band (i.e. the best-fit magnitude of a point-like object fit to the pixel data)\n",
    "* psfrmag:      PSF magnitude in r-band (i.e. the best-fit magnitude of a point-like object fit to the pixel data)\n",
    "* psfimag:      PSF magnitude in i-band (i.e. the best-fit magnitude of a point-like object fit to the pixel data)\n",
    "* W3:   magnitude in W3-band (from AllWISE)\n",
    "* W3err:        uncertainty on magnitude in W1-band (from AllWISE)\n",
    "* J:    magnitude in J-band (from 2MASS, in AllWISE)\n",
    "* Jerr: uncertainty on J\n",
    "* H:    magnitude in J-band (from 2MASS, in AllWISE)\n",
    "* Herr: uncertainty on H\n",
    "* K:    magnitude in J-band (from 2MASS, in AllWISE)\n",
    "* Kerr: uncertainty on K\n",
    "* umag: magnitude in u-band\n",
    "* zs: \"true\" redshift\n",
    "\n",
    "Make sure that you shortly think about (and discuss) which of these features should be included, if you want to try to identify which type of object it is.\n",
    "\n",
    "Also, this time there is no target value (Y) given in the data. However, given the query selection (by other means) to be stars, galaxies, and quasars, you can consider the file type to be the target. But you need to put these three files together and add a column with the target (i.e. file origin) value.\n",
    "\n",
    "\n",
    "### Task:\n",
    "Thus, the task before you is to:<br>\n",
    "1) Make three queries, which produces three files of data containing stars, galaxies, and quasars.<br>\n",
    "2) Combine the three data files into one, which has a target value corresponding to the file type.<br>\n",
    "3) Read and inspect this data, and make sure that you understand what it (roughly) looks like.<br>\n",
    "4) Clean/cut (or impute) the data, such that different (unsupervised) analysis techniques will work.<br>\n",
    "5) Run a (k)PCA (and later other techniques) on it, and see what the resulting distributions looks like.<br>\n",
    "\n",
    "Do you in the end manage to get e.g. get three well separated classes out?<br>\n",
    "\n",
    "***\n",
    "\n",
    "* Author: Troels C. Petersen (NBI)\n",
    "* Email:  petersen@nbi.dk\n",
    "* Date:   3rd of May 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# IF PLOTS ARE WEIRD INSTALL  ipympl OR REMOVE '%matplotlib widget' \N"
    "###### PCA / KPCA Example\n",
    "#  Author: Rasmus F. Ørsøe \n",
    "#\n",
    "#\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from copy import deepcopy\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def MakeMyPlots(components, truth, include_3d):\n",
    "    \n",
    "    # 2D Plots\n",
    "    fig, ax = plt.subplots(components.shape[1], components.shape[1], figsize = (15,5))\n",
    "    for i in range(0,components.shape[1]):\n",
    "        for j in range(0,components.shape[1]):\n",
    "            if j!=i:\n",
    "                ax[i,j].scatter(components[truth_data['truth'] == 0,i],components[truth_data['truth'] == 0,j], label = 'galaxy')\n",
    "                ax[i,j].scatter(components[truth_data['truth'] == 1,i],components[truth_data['truth'] == 1,j], label = 'quasar')\n",
    "                ax[i,j].scatter(components[truth_data['truth'] == 2,i],components[truth_data['truth'] == 2,j], label = 'stars')\n",
    "                ax[i,j].legend()\n",
    "                ax[i,j].set_ylabel('Component %s'%j)\n",
    "                ax[i,j].set_xlabel('Component %s'%i)\n",
    "                ax[i,j].set_xticks([])\n",
    "                ax[i,j].set_yticks([])\n",
    "    if include_3d:\n",
    "        #3D plot           \n",
    "        fig = plt.figure( figsize = (5,5))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(components[truth_data['truth'] == 0,0],components[truth_data['truth'] == 0,1],components[truth_data['truth'] == 0,2], label = 'galaxy')\n",
    "        ax.scatter(components[truth_data['truth'] == 1,0],components[truth_data['truth'] == 1,1],components[truth_data['truth'] == 1,2], label = 'quasar')\n",
    "        ax.scatter(components[truth_data['truth'] == 2,0],components[truth_data['truth'] == 2,1],components[truth_data['truth'] == 2,2], label = 'stars')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Comp. 0')\n",
    "        ax.set_ylabel('Comp. 1')\n",
    "        ax.set_zlabel('Comp. 2')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's start by loading the data and doing a quick PCA without cleaning anything, besides removing 'ID', 'ra' and 'dec' as we were told these shouldn't contain important information. Because we have three classes, I'll ask the PCA for three components. Note: Because this is unsupervised learning, we're not guaranteed that PCA will split the data according to the classes that we hope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies = pd.read_csv('Data_Galaxies.txt', comment = '#').reset_index(drop = True)\n",
    "quasars = pd.read_csv('Data_Quasars.txt', comment = '#').reset_index(drop = True)\n",
    "stars = pd.read_csv('Data_Stars.txt', comment = '#').reset_index(drop = True)\n",
    "galaxies['truth'] = 0\n",
    "quasars['truth'] = 1\n",
    "stars['truth'] = 2\n",
    "\n",
    "data = pd.concat([galaxies,quasars,stars], axis = 0, ignore_index = True).reset_index(drop = True) \n",
    "\n",
    "# Notice that I didn't do the pair plot of the variables. This is usually a good idea - but in this case - there's so many variables that the pairplot becomes huge. \n",
    "# sns.pairplot(data.sample(frac = 0.1), hue='truth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fae4321fd774e67b1781bd59da64044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2063cf1afe3e4f21b409892f6d7a53e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_variables = ['truth', 'zs'] #Truth is our classificaiton label, 'zs' is a regression target that we'll completely ignore in this example.\n",
    "input_variables = data.columns[(data.columns != 'truth') & (data.columns != 'zs') & (data.columns != 'ID') & (data.columns != 'ra') & (data.columns != 'dec')]\n",
    "input_data = data.loc[:,input_variables]\n",
    "truth_data = data.loc[:,target_variables]\n",
    "\n",
    "components = PCA(n_components = 3).fit_transform(input_data)\n",
    "\n",
    "MakeMyPlots(components,truth_data, include_3d = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some separation.. But it isn't great. Let's see what happens if we instead try KPCA. You can find the documentation for it here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "\n",
    "as you can see, for the kernel type we can choose: kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘cosine’, ‘precomputed’}, default=’linear’\n",
    "\n",
    "I'll pick 'cosine'. What do you think we'll get if we choose 'linear' instead?\n",
    "\n",
    "Warning: This is memory intensive! If you have less than ~16gb RAM you should probably do this on a (random) subset of the data. You can get a random subsample very quickly by subset =  data.sample(frac = 0.3) to get a random sample of 30% of the original size. \n",
    "\n",
    "(if you run this on ERDA - don't worry :-) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6080a2c2e442d1b0070505cc21124e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e883a33913704cecb13e1567848f009d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer = KernelPCA(n_components=3, kernel='cosine')\n",
    "components = transformer.fit_transform(input_data)\n",
    "\n",
    "MakeMyPlots(components, truth_data, include_3d = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! From the 2D-plots one can quickly get the impression that the seperation is OK. But if you play around with the 3D plot you can see, unfortunately, that many of the classes ends up plotted on top of each other anyway. So what can we do? We can try to fix the data - e.g. to preprocess it somehow such that the seperation becomes easier for KPCA. Adriano told us that values of 9999.000 are put in as placeholder values. Let's print out data to check how this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             istar      gmag      rmag      imag      zmag      W1      W2  \\\n",
      "0       -28.583860  22.80513  20.86707  19.86073  19.40347  15.624  15.748   \n",
      "1       -28.201030  22.61204  20.91495  19.98573  19.42935  15.631  15.854   \n",
      "2     -6275.146000  18.34751  17.21329  16.71394  16.31988  13.916  13.749   \n",
      "3       -19.421730  22.34108  20.65322  19.75191  19.19925  15.872  15.847   \n",
      "4     -2156.052000  18.68034  17.69513  17.26544  16.89467  14.558  14.451   \n",
      "...            ...       ...       ...       ...       ...     ...     ...   \n",
      "29995    -2.785142  19.04584  17.62281  16.71206  16.23755  13.954  13.929   \n",
      "29996   -46.915640  18.00692  17.33035  17.08265  16.99487  15.410  15.403   \n",
      "29997   -18.991050  21.02905  19.46825  18.06293  17.28976  14.822  14.947   \n",
      "29998  -210.847900  15.61581  15.09167  14.93607  14.87516  13.504  13.550   \n",
      "29999   -73.580420  17.88042  16.45881  15.18252  14.50631  12.128  11.981   \n",
      "\n",
      "        psfgmag   psfrmag   psfimag      W3     W3err         J      Jerr  \\\n",
      "0      23.05673  21.22807  20.27738  12.717  9999.000  9999.000  9999.000   \n",
      "1      23.49406  21.80100  20.85607  12.276  9999.000  9999.000  9999.000   \n",
      "2      19.69872  18.74084  18.31459  12.361     0.391    15.932     0.092   \n",
      "3      22.82141  21.31727  20.44886  12.640  9999.000  9999.000  9999.000   \n",
      "4      19.67504  18.66901  18.25492  12.032  9999.000    16.035     0.089   \n",
      "...         ...       ...       ...     ...       ...       ...       ...   \n",
      "29995  19.02711  17.62426  16.71595  13.040  9999.000    14.946     0.040   \n",
      "29996  18.00322  17.33501  17.06419  11.918  9999.000    16.252     0.105   \n",
      "29997  21.02102  19.46401  18.07509  12.434  9999.000    15.904     0.075   \n",
      "29998  15.60797  15.07403  14.92410  12.259  9999.000    13.972     0.028   \n",
      "29999  17.87350  16.46019  15.18078  11.872     0.309    13.141     0.024   \n",
      "\n",
      "              H      Herr         K      Kerr      umag  \n",
      "0      9999.000  9999.000  9999.000  9999.000  23.42103  \n",
      "1      9999.000  9999.000  9999.000  9999.000  22.55242  \n",
      "2        14.987     0.081    14.563     0.109  20.42177  \n",
      "3      9999.000  9999.000  9999.000  9999.000  24.11596  \n",
      "4        15.456     0.135    15.115     0.142  20.62453  \n",
      "...         ...       ...       ...       ...       ...  \n",
      "29995    14.364     0.037    14.073     0.050  21.74448  \n",
      "29996    15.438     0.124    15.819     0.221  19.69802  \n",
      "29997    15.206     0.105    15.185     0.148  23.24605  \n",
      "29998    13.576     0.029    13.566     0.044  17.23314  \n",
      "29999    12.566     0.027    12.290     0.019  20.44086  \n",
      "\n",
      "[30000 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "print(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this little quick inspection we can see that 9999.000 seems to be quite a big value for some of these variables - compare, for an example, row 0 and row 2 for variable 'Kerr':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0: 9999.0\n",
      "row 2: 0.109\n"
     ]
    }
   ],
   "source": [
    "print('row 0: %s'%input_data['Kerr'][0])\n",
    "print('row 2: %s'%input_data['Kerr'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were training a neural network, having such big numerical differences within a single variable would be quite troublesome. Would it matter for a decision tree? As mentioned in the lectures, there's quite a few ways to attack this problem. The three most obvious (to me) are:\n",
    "\n",
    "1) Remove any variables that have a high density of 9999.000 values\n",
    "2) Remove any rows that have 9999.000 in any variable\n",
    "3) Transform the data such that the numerical differences become smaller.\n",
    "\n",
    "\n",
    "Let's explore the first option first. Let's try to find out how many variables we have left, if we drop any variable that has more than 30% of these 9999.000 values, and plot the KPCA for this selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping:\n",
      "['W3err', 'J', 'Jerr', 'H', 'Herr', 'K', 'Kerr']\n",
      "keeping:\n",
      "Index(['istar', 'gmag', 'rmag', 'imag', 'zmag', 'W1', 'W2', 'psfgmag',\n",
      "       'psfrmag', 'psfimag', 'W3', 'umag'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6801ec93657a4484b6e38e783bf53e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153ff73144304a379d5a42970995e54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "variables_to_drop = []\n",
    "threshold = 0.3\n",
    "for variable in input_data.columns:\n",
    "    n_9999 = sum(input_data[variable] == 9999.000)\n",
    "    if n_9999/len(input_data[variable]) > threshold:\n",
    "        variables_to_drop.append(variable)\n",
    "print('dropping:')\n",
    "print(variables_to_drop)\n",
    "clean_data = input_data.drop(variables_to_drop, axis = 1)\n",
    "\n",
    "print('keeping:')\n",
    "print(clean_data.columns)\n",
    "transformer = KernelPCA(n_components=3, kernel='cosine')\n",
    "components = transformer.fit_transform(clean_data)\n",
    "MakeMyPlots(components, truth_data, include_3d = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't seem like a super choice. Let's instead try to explore the second option by find the number of rows containing this 9999.000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in sample:\n",
      "30000\n",
      "number of rows to remove:\n",
      "26737\n"
     ]
    }
   ],
   "source": [
    "rows_to_drop = 0\n",
    "for row in range(0,len(input_data)):     # Looping over columns is much quicker, but I thought this was easier to read.\n",
    "    current_row = input_data.loc[row, :]  \n",
    "    \n",
    "    if sum(current_row == 9999.000) > 0: # This works because True + False =  1. So if there's just one or more True in (current_row == 9999.000) we increment rows_to_drop by 1.\n",
    "        rows_to_drop +=1\n",
    "print('number of rows in sample:')\n",
    "print(len(input_data))\n",
    "print('number of rows to remove:')\n",
    "print(rows_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! That's nearly all rows! Let's forget that approach. Let's try the third option -  a transformation!  You can read much more about preprocessing here : https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "Out of habit, I'll choose sklearn.preprocessing.RobustScaler() as my transformer. I'll fit and transform to every variable in the data, and plot the KPCA results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 19)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e75793525e446cbab7a98d719550555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9062f7bae27742e9ab96de4a43890cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformed_data = deepcopy(input_data) \n",
    "# This loop transforms every variable _independently_ \n",
    "for variable in input_data.columns:     \n",
    "    transformed_data[variable] = RobustScaler().fit_transform(np.array(transformed_data[variable]).reshape(-1,1))\n",
    "\n",
    "print(transformed_data.shape)\n",
    "transformer = KernelPCA(n_components=3, kernel='cosine')\n",
    "components = transformer.fit_transform(transformed_data)\n",
    "\n",
    "MakeMyPlots(components, truth_data, include_3d = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not too bad!\n",
    "\n",
    "With a bit of effort you could probably produce an even better result. You could try:\n",
    "\n",
    "A) Different transformers \\\n",
    "B) A combination of 1) and 3) \\\n",
    "C) Different kernel types \\\n",
    "D) A completely different method than KPCA! (Pssst: sklearn.manifold.TSNE, PsstPsst: https://umap-learn.readthedocs.io/en/latest/basic_usage.html)\n",
    "\n",
    "\n",
    "Finally:\n",
    "Do you think the result would improve, become worse or stay the same if we kept the variables ('ID', 'ra', 'dec'), which we threw away right in the beginning?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
